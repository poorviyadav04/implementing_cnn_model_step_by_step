{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0acf18bb-e71f-4331-b1dc-e03324ca4800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f3aaad-7fa7-4bab-b948-055670266812",
   "metadata": {},
   "source": [
    "ZERO PADDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd09010f-d976-45b8-bc1a-5c45ea5959f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2 * pad, n_W + 2 * pad, n_C)\n",
    "    \"\"\"\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), mode='constant', constant_values=0)\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d51791c-df22-4f32-9e6f-b94d6646751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =\n",
      " (4, 3, 3, 2)\n",
      "x_pad.shape =\n",
      " (4, 9, 9, 2)\n",
      "x[1,1] =\n",
      " [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] =\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x163102f6270>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 3)\n",
    "print (\"x.shape =\\n\", x.shape)\n",
    "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
    "print (\"x[1,1] =\\n\", x[1, 1])\n",
    "print (\"x_pad[1,1] =\\n\", x_pad[1, 1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0, :, :, 0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490f8a2-389e-4d00-b696-3852d13014fb",
   "metadata": {},
   "source": [
    "IMPLEMENTING CONV SINGLE STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbee3504-5ab3-4910-81d2-dc789e7be7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "    s = a_slice_prev * W                      # element-wise product\n",
    "    Z = np.sum(s)                             # sum over all elements\n",
    "    Z = Z + float(b)                          # add bias     \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "701eea6f-53ab-447e-861d-afb0376096c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.999089450680221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yadav\\AppData\\Local\\Temp\\ipykernel_14156\\31009551.py:18: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Z = Z + float(b)                          # add bias\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc72a6-7d26-44fd-a114-5f3fad26aa65",
   "metadata": {},
   "source": [
    "IMPLEMENTING CONV FORWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bdb5b08-eaaf-47c8-b142-7230a476b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, \n",
    "              numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape  # Retrieve dimensions from A_prev's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape                   # Retrieve dimensions from W's shape\n",
    "    stride = hparameters[\"stride\"]                   # Retrieve information from \"hparameters\"\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1    # Compute the dimensions of the CONV output volume\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    Z = np.zeros((m, n_H, n_W, n_C))                    # Initialize the output volume Z with zeros\n",
    "    A_prev_pad = zero_pad(A_prev, pad)                  # Create A_prev_pad by padding A_prev\n",
    "    \n",
    "    # Loop over the batch of training examples\n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]                   # Select ith training example’s padded activation\n",
    "        \n",
    "        for h in range(n_H):                         # Loop over vertical axis\n",
    "            vert_start = h * stride\n",
    "            vert_end = vert_start + f\n",
    "                \n",
    "            for w in range(n_W):                     # Loop over horizontal axis\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range(n_C):                 # Loop over channels (= #filters)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]       # Get the 3D slice\n",
    "                    weights = W[:, :, :, c]           # Get filter and bias\n",
    "                    biases = b[:, :, :, c]\n",
    "            \n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)     # Convolve the slice\n",
    "    \n",
    "    cache = (A_prev, W, b, hparameters)    #Save info in cache for bp\n",
    "    \n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee5f6557-46b8-46fa-94fc-e559a2aa321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean =\n",
      " 0.5511276474566768\n",
      "Z[0,2,1] =\n",
      " [-2.17796037  8.07171329 -0.5772704   3.36286738  4.48113645 -2.89198428\n",
      " 10.99288867  3.03171932]\n",
      "cache_conv[0][1][2][3] =\n",
      " [-1.1191154   1.9560789  -0.3264995  -1.34267579]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yadav\\AppData\\Local\\Temp\\ipykernel_14156\\31009551.py:18: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Z = Z + float(b)                          # add bias\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 7, 4)\n",
    "W = np.random.randn(3, 3, 4, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\" : 1,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "z_mean = np.mean(Z)\n",
    "z_0_2_1 = Z[0, 2, 1]\n",
    "cache_0_1_2_3 = cache_conv[0][1][2][3]\n",
    "print(\"Z's mean =\\n\", z_mean)\n",
    "print(\"Z[0,2,1] =\\n\", z_0_2_1)\n",
    "print(\"cache_conv[0][1][2][3] =\\n\", cache_0_1_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b54b57-298b-46da-b744-170faa2d5583",
   "metadata": {},
   "source": [
    "IMPLEMENTING MAX POOLING AND AVERAGE POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "612874a4-49ec-43d4-b472-b821db15b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode=\"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- pooling mode: \"max\" or \"average\"\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters\n",
    "    \"\"\"\n",
    "\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape     # Retrieve dimensions from the input shape\n",
    "    f = hparameters[\"f\"]                                  # Retrieve hyperparameters\n",
    "    stride = hparameters[\"stride\"]\n",
    "\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)               # Compute dimensions of the output\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    A = np.zeros((m, n_H, n_W, n_C))                     # Initialize output matrix A\n",
    "    \n",
    "    for i in range(m):                         # loop over the batch\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):                   # vertical axis\n",
    "            vert_start = h * stride\n",
    "            vert_end = vert_start + f\n",
    "            for w in range(n_W):               # horizontal axis\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                for c in range(n_C):           # channel axis\n",
    "                    a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "                        \n",
    "    cache = (A_prev, hparameters)      # Store input and hparameters for backprop\n",
    "    \n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6937c74-c804-478b-96c0-bc2490a63a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE 1:\n",
      "\n",
      "mode = max\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A[1, 1] =\n",
      " [[1.96710175 0.84616065 1.27375593]\n",
      " [1.96710175 0.84616065 1.23616403]\n",
      " [1.62765075 1.12141771 1.2245077 ]]\n",
      "mode = average\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A[1, 1] =\n",
      " [[ 0.44497696 -0.00261695 -0.31040307]\n",
      " [ 0.50811474 -0.23493734 -0.23961183]\n",
      " [ 0.11872677  0.17255229 -0.22112197]]\n",
      "\n",
      "\u001b[0mCASE 2:\n",
      "\n",
      "mode = max\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A[0] =\n",
      " [[[1.74481176 0.90159072 1.65980218]\n",
      "  [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      " [[1.13162939 1.51981682 2.18557541]\n",
      "  [1.13162939 1.6924546  2.18557541]]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A[1] =\n",
      " [[[-0.17313416  0.32377198 -0.34317572]\n",
      "  [ 0.02030094  0.14141479 -0.01231585]]\n",
      "\n",
      " [[ 0.42944926  0.08446996 -0.27290905]\n",
      "  [ 0.15077452  0.28911175  0.00123239]]]\n"
     ]
    }
   ],
   "source": [
    "# Case 1: stride of 1\n",
    "print(\"CASE 1:\\n\")\n",
    "np.random.seed(1)\n",
    "A_prev_case_1 = np.random.randn(2, 5, 5, 3)\n",
    "hparameters_case_1 = {\"stride\" : 1, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev_case_1, hparameters_case_1, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1, 1] =\\n\", A[1, 1])\n",
    "A, cache = pool_forward(A_prev_case_1, hparameters_case_1, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1, 1] =\\n\", A[1, 1])\n",
    "\n",
    "\n",
    "# Case 2: stride of 2\n",
    "print(\"\\n\\033[0mCASE 2:\\n\")\n",
    "np.random.seed(1)\n",
    "A_prev_case_2 = np.random.randn(2, 5, 5, 3)\n",
    "hparameters_case_2 = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev_case_2, hparameters_case_2, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[0] =\\n\", A[0])\n",
    "print()\n",
    "\n",
    "A, cache = pool_forward(A_prev_case_2, hparameters_case_2, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1] =\\n\", A[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0877552-3183-49ca-8db4-30a6e440f9d3",
   "metadata": {},
   "source": [
    "IMPLEMENTING CONV BACKWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "454b543d-8561-4624-ac17-65105ce5ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function.\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), \n",
    "          numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values (A_prev, W, b, hparameters) from conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    (A_prev, W, b, hparameters) = cache # Retrieve information from \"cache\"\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape  # Retrieve dimensions from A_prev's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape     # Retrieve dimensions from W's shape\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "\n",
    "    (m, n_H, n_W, n_C) = dZ.shape    # Retrieve dimensions from dZ's shape\n",
    "    \n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))     # Initialize dA_prev, dW, db with the correct shapes                      \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "    \n",
    "    A_prev_pad = zero_pad(A_prev, pad)    # Pad A_prev and dA_prev\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]   # Slice a_prev_pad\n",
    "\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]  # Update gradients\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "    \n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :] if pad != 0 else da_prev_pad  # Unpad da_prev_pad\n",
    "    \n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dae350c-2f9b-4eb1-be19-cc1dbe96f460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 1.4524377775388075\n",
      "dW_mean = 1.7269914583139097\n",
      "db_mean = 7.839232564616838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yadav\\AppData\\Local\\Temp\\ipykernel_14156\\31009551.py:18: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Z = Z + float(b)                          # add bias\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10, 4, 4, 3)\n",
    "W = np.random.randn(2, 2, 3, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd14ac-15f7-4dda-a383-83dae1894f8f",
   "metadata": {},
   "source": [
    "POOLING LAYER - BACKWARD PASS\n",
    "this function creates a \"mask\" matrix which keeps track of where the maximum of the matrix is. True (1) indicates the position of the maximum in X, the other entries are False (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6773f737-7130-4992-806b-df6bbf8a5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as x, contains a True at the position \n",
    "            corresponding to the max entry of x.\n",
    "    \"\"\"    \n",
    "\n",
    "    mask = (x == np.max(x))\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa57fee9-24d1-4bfa-91f5-507214f3d72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2, 3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)\n",
    "\n",
    "x = np.array([[-1, 2, 3],\n",
    "              [2, -3, 2],\n",
    "              [1, 5, -2]])\n",
    "\n",
    "y = np.array([[False, False, False],\n",
    "     [False, False, False],\n",
    "     [False, True, False]])\n",
    "mask = create_mask_from_window(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c5ad35-82c6-4896-b579-6c237998bad9",
   "metadata": {},
   "source": [
    "AVERAGE POOLING- BACKWARD PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee1e2b05-0e29-48e8-840c-279cd77f91dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape.\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar (the gradient from the next layer)\n",
    "    shape -- tuple of integers (n_H, n_W), the shape of the output matrix \n",
    "             for which we want to distribute the value of dz.\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) with each value equal to dz/(n_H*n_W)\n",
    "    \"\"\"    \n",
    "\n",
    "    (n_H, n_W) = shape\n",
    "    average = dz / (n_H * n_W)\n",
    "    a = np.ones((n_H, n_W)) * average   # Create a matrix of average value\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7bc2fe2-9979-4288-8b35-368cd53614b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2, 2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d3d55-8842-45bc-8409-063f65fc688c",
   "metadata": {},
   "source": [
    "Putting it Together: Pooling Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d6c8f36-fd3c-4b41-a367-99a4208234d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode=\"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains (A_prev, hparameters)\n",
    "    mode -- pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    (A_prev, hparameters) = cache \n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (m, n_H, n_W, n_C) = dA.shape\n",
    "    \n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i]\n",
    "        \n",
    "        for h in range(n_H):                   # vertical axis\n",
    "            for w in range(n_W):               # horizontal axis\n",
    "                for c in range(n_C):           # channels\n",
    "                \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c] # Slice from a_prev\n",
    "                        mask = create_mask_from_window(a_prev_slice)                         # Create mask\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]   # Distribute the gradient only where the max was\n",
    "                    \n",
    "                    elif mode == \"average\":\n",
    "                        da = dA[i, h, w, c]  # Get the gradient from output\n",
    "                        shape = (f, f)      # Define filter shape\n",
    "                        distributed = distribute_value(da, shape)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distributed  # Add to dA_prev slice\n",
    "                    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "151ad37f-1058-4c1b-9973-790e849e17e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4, 2, 2)\n",
      "(5, 5, 3, 2)\n",
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev1[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev2[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(A.shape)\n",
    "print(cache[0].shape)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev1 = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev1[1,1] = ', dA_prev1[1, 1])  \n",
    "print()\n",
    "dA_prev2 = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev2[1,1] = ', dA_prev2[1, 1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1253f77-9d54-4d0b-b2da-850be52b15c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
